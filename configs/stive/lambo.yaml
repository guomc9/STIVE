pretrained_t2v_model_path: "./checkpoints/zeroscope_v2_576w"
pretrained_concepts_model_path: "./checkpoints/concepts/lambo"
output_dir: ".log/stive/lambo/base/"
checkpoints_dir: "checkpoints/stive/lambo/"

train_data:
  video_paths:
    # - "data/targets/car-turn/videos/car-turn.mp4"
    - "data/concepts/lambo/videos/lambo.mp4"
  prompts:
    # - "a jeep driving down a curvy road in the countryside"
    # - "a jeep driving on the road"
    - "a $LAMBO is driving along a road"
  num_frames: 8
  sample_stride: 2
  width: 512
  height: 512
  rand_slice: True
  target_video_paths:
    - "data/targets/car-turn/videos/car-turn.mp4"
  target_mask_paths:
    - "data/targets/car-turn/masks/car-turn_jeep_frame_masks.mp4"
  target_prompts:
    - "a $ LAMBO driving down a curvy road in the countryside"

validation_data:
  source: "data/targets/car-turn/videos/car-turn.mp4"
  source_prompt: "a jeep driving down a curvy road in the countryside"
  # source_prompt: "a jeep driving on the road"
  edit_prompts:
    # - "a jeep driving down a curvy road in the countryside"
    - "a $LAMBO driving down a curvy road in the countryside"
    # - "a jeep driving on the road"
    # - "a $LAMBO driving on the road"
  num_frames: 8
  sample_stride: 3
  width: 512
  height: 512

lora_conf:
  r: 8
  lora_alpha: 1.0
  lora_dropout: 0.1
  target_modules:
    - "^.*\\.attentions\\..*\\.attn1\\.to_q\\..*$"
    - "^.*\\.attentions\\..*\\.attn2\\.to_q\\..*$"
    # - "^.*\\.attentions\\..*\\.attn2\\.to_v\\..*$"
    - "^.*\\.attentions\\..*\\.ff\\.net\\.2\\..*$"
  bias: "none"

# extra_trainable_modules:
#   - "^.*\\.attentions\\..*\\.attn1\\.to_q\\..*$"
#   - "^.*\\.attentions\\..*\\.attn2\\.to_q\\..*$"



inference_conf:
  num_inference_steps: 30
  guidance_scale: 12.5
  use_inv_latent: True
  num_inv_steps: 30


# # CHECK
# sub_sot: True
# cam_loss_type: 'mae'
# enable_scam_loss: True
# scam_weight: 0.0
# enable_tcam_loss: True
# tcam_weight: 0.0
# attn_check_steps: 10

learning_rate: 1.0e-4
batch_size: 1
num_train_epoch: 300
checkpointing_steps: 2000
validation_steps: 50

seed: 33
mixed_precision: 'fp16'
use_8bit_adam: False
gradient_checkpointing: True
enable_xformers_memory_efficient_attention: True
enable_torch_2_attn: True
gradient_accumulation_steps: 1
